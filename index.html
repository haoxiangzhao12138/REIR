<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Referring Expression Instance Retrieval and A Strong End-to-End Baseline</title>
  <link rel="icon" type="image/x-icon" href="./static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://code-kunkun.github.io/" target="_blank">Xiangzhao Hao</a><sup>1</sup>, </span>
                <span class="author-block">
                Kuan Zhu<sup>1</sup>, </span>
                <span class="author-block">
                Hongyu Guo<sup>1</sup>, </span>
                <span class="author-block">
                Haiyun Guo<sup>1</sup>, </span>
                <span class="author-block">
                Ning Jiang<sup>2</sup>, </span>
                <br>
                <span class="author-block">
                Quan Lu<sup>2</sup>, </span>
                <span class="author-block">
                Ming Tang<sup>1</sup>, </span>
                <span class="author-block">
                Jinqiao Wang<sup>1</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Institute of Automation, Chinese Academy of Sciences</span><br>
                    <span class="author-block"><sup>2</sup>CMashang Consumer Finance Co, Ltd</span>&nbsp;&nbsp;&nbsp;
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2506.18246" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary dataset link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/haoxiangzhao/REIRCOCO" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/haoxiangzhao12138/REIR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.18246" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="vertical-image-display">
        <!-- <div class="columns is-centered has-text-centered"><h2 class="title"></h2></div> -->
        <div class="image-item">
          <img src="./static/images/teaser_figure.png" alt="Teaser"/>
          <h2 class="subtitle">
          <figcaption>
            Comparison of three vision-language tasks and their datasets. <strong>REIR enables end-to-end instance-level retrieval and localization</strong>, addressing the limitations of Text-Image retrieval and Referring Expression Comprehension.
          </figcaption>
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero ">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
            <p>
                Natural language querying of visual information is fundamental to many real-world applications, motivating a broad range of vision-language tasks. 
                <strong>Text-Image Retrieval (TIR)</strong> retrieves an image based on an image-level description, while <strong>Referring Expression Comprehension (REC)</strong> localizes a target object in an image using an instance-level description. 
                However, neither can fully handle real-world demands where users often search for specific object instances across large galleries and expect both the relevant image and the exact object location.
              </p>
              
              <p>
                We introduce a new task, <strong>Referring Expression Instance Retrieval (REIR)</strong>, which jointly supports instance-level retrieval and localization based on fine-grained referring expressions. 
                To support this task, we construct <strong>REIRCOCO</strong>, a large-scale benchmark with high-quality expressions generated via prompting vision-language foundation models on MSCOCO and RefCOCO.
              </p>
              
              <p>
                We also propose <strong>CLARE</strong> (Contrastive Language-Instance Alignment with Relation Experts), a dual-stream model designed for REIR. 
                It leverages a vision branch for extracting instance-level object features and a language branch enhanced by a <strong>Mix of Relation Experts (MORE)</strong> module. 
                Retrieval and localization are jointly optimized using a novel <strong>CLIA</strong> contrastive alignment objective. 
                Experiments show CLARE outperforms strong baselines on REIR and generalizes well to TIR and REC tasks.
              </p>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Vertical Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="vertical-image-display">
        <div class="columns is-centered has-text-centered"><h2 class="title is-3">Dataset</h2></div>
        <div class="image-item">
          <img src="./static/images/dataset.png" alt="Overall Structure"/>
          <h2 class="subtitle">
            <figcaption>
                <strong>Overview of the REIRCOCO dataset construction pipeline.</strong>
                REIRCOCO is a large-scale benchmark specifically designed for instance-level retrieval and localization.
                It features uniquely aligned referring expressions for over 215,000 object instances in 30,000+ images, totaling 613,000 fine-grained descriptions.
                The dataset is constructed through a two-stage pipeline: 
                In the <strong>generation stage</strong>, GPT-4o is prompted with structured inputs—including bounding boxes, category labels, captions, and object context—to generate diverse and referentially unique expressions.
                In the <strong>filtering stage</strong>, DeepSeek-VL verifies expression quality, retaining only unambiguous, grounded, and semantically accurate descriptions.
                This ensures that each expression matches exactly one object instance, making REIRCOCO highly suitable for both retrieval and localization tasks.
              </figcaption>
          </h2>
        </div>
        <div class="columns is-centered has-text-centered"><h2 class="title is-3">Method</h2></div>
        <div class="image-item">
          <img src="./static/images/method.png" alt="Overall Structure"/>
          <h2 class="subtitle">
            <figcaption>
                <strong>Overview of the proposed CLARE framework.</strong> 
                CLARE (Contrastive Language-Instance Alignment with Relation Experts) is a dual-stream architecture designed for instance-level retrieval and localization.
                It processes vision and language in parallel while maintaining strong cross-modal alignment.
                On the visual side, a SigLIP encoder and a Deformable-DETR-based object extractor generate dense, context-aware object features.
                The language side uses a referring expression encoder enhanced by the Mix of Relation Experts (MORE) module, which injects semantic, spatial, and relational cues.
                Cross-image instance-level alignment is supervised by the CLIA objective, which extends SigLIP’s contrastive loss for object-level understanding.
                Focal Loss is further used to enhance discriminative instance selection within each image.
                Together, these components enable high-precision retrieval and localization across large-scale galleries.
              </figcaption>
          </h2>
        </div>
        <div class="columns is-centered has-text-centered"><h2 class="title is-3">Results</h2></div>
        <!-- 图1：REIR任务上的性能比较 -->
        <div class="image-item">
            <img src="./static/images/reir_results.png" alt="REIR Performance Comparison" />
            <h2 class="subtitle">
                <figcaption>
                    <strong>CLARE achieves state-of-the-art performance on the REIR benchmark.</strong>
                    As illustrated in Table 1, CLARE consistently outperforms all two-stage baselines across various IoU thresholds and ranking metrics. 
                    Unlike conventional pipelines that separate retrieval and localization—leading to error accumulation—CLARE integrates both tasks into a single, unified framework.
                    This end-to-end design enables more precise cross-modal alignment, improves robustness, and significantly reduces inference cost.
                  </figcaption>
            </h2>
        </div>
        
        <!-- 图2：REC任务上的性能比较 -->
        <div class="image-item">
            <img src="./static/images/rec_results.png" alt="REC Benchmark Comparison" />
            <h2 class="subtitle">
                <figcaption>
                    <strong>CLARE demonstrates competitive results on REC benchmarks (RefCOCO, RefCOCO+, RefCOCOg).</strong>
                    Despite its dual-stream architecture, CLARE matches or outperforms strong one-stream and MLLM-based methods. 
                    Its success stems from a powerful instance-level contrastive alignment strategy, which enables effective semantic and spatial matching across the batch.
                    CLARE excels on RefCOCO and RefCOCOg, where relational and spatial cues are crucial, and remains robust even on RefCOCO+, where such cues are reduced—showcasing strong generalization ability across diverse REC settings.
                  </figcaption>
            </h2>
        </div>
        <div class="columns is-centered has-text-centered"><h2 class="title is-3">Qualitative Results</h2></div>
        <div class="image-item">
          <img src="./static/images/qualitative_result.png" alt="Qualitative Results"/>
        </div>
        <div class="image-item">
            <img src="./static/images/qualitative_result2.png" alt="Qualitative Results"/>
          </div>
      </div>
    </div>
  </div>
</section>
<!-- End Vertical Image Display -->

<style>
  .subtitle {
    width: 70%;
    display: block;
    margin-left: calc(50% - 35%);
  }
  .vertical-image-display .image-item {
    margin-bottom: 50px; /* Space between items */
  }
  .vertical-image-display img {
    width: 70%; /* Adjust width as needed */
    height: auto; /* Maintain aspect ratio */
    /* Centering image */
    display: block;
    margin-left: calc(50% - 35%);
  }
  figcaption {
    font-size: 14px; 
    line-height: 1.5;
  }
  .hero.is-small {
      margin-bottom: -5rem; 
      padding-bottom: 0;
    }
</style>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hao2025referring,
        title={Referring Expression Instance Retrieval and A Strong End-to-End Baseline},
        author={Hao, Xiangzhao and Zhu, Kuan and Guo, Hongyu and Guo, Haiyun and Tang, Ming and Wang, JinQiao},
        journal={arXiv preprint arXiv:2506.18246},
        year={2025}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>